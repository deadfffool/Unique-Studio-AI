{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from collections import OrderedDict \n",
    "from collections import defaultdict\n",
    "import math\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'and',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'fun',\n",
       " 'and',\n",
       " 'exciting']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"natural language processing and machine learning is fun and exciting\"\n",
    "corpus = [word.lower() for word in text.split()]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    'window_size':2,\n",
    "    'n':10, #单词嵌入的维度\n",
    "    'epochs': 50,\n",
    "    'learning_rate':0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vec():\n",
    "    def __init__(self):\n",
    "        self.n = settings['n']\n",
    "        self.lr = settings['learning_rate']\n",
    "        self.epochs = settings['epochs']\n",
    "        self.window = settings['window_size']\n",
    "    def generate_training_data(self,settings,corpus):\n",
    "        word_counts = defaultdict(int)\n",
    "        for row in corpus:\n",
    "            for word in row:\n",
    "                word_counts[word] +=1\n",
    "        self.v_count = len(word_counts.keys())\n",
    "        self.words_list = list(word_counts.keys())\n",
    "        self.word_index = dict((word,i)for i ,word in enumerate(self.words_list))\n",
    "        self.index_word = dict((i,word)for i ,word in enumerate(self.words_list))\n",
    "\n",
    "        trainging_data = []\n",
    "        for sentence in corpus:\n",
    "            sent_len= len(sentence)\n",
    "            for i ,word in enumerate(sentence):\n",
    "                w_target = self.word2onehot(sentence[i])\n",
    "                word_context = []\n",
    "                for j in range (i-self.window,i+self.window+1):\n",
    "                    if j != i and j <= sent_len-1 and j > 0:\n",
    "                        word_context.append(self.word2onehot(sentence[j]))\n",
    "                    trainging_data.append([w_target,word_context])\n",
    "        return np.array(trainging_data)\n",
    "\n",
    "    def word2onehot(self,word):\n",
    "        word_vec = [0 for i in range(0,self.v_count)]\n",
    "        word_index = self.word_index[word]\n",
    "        word_vec[word_index]=1 \n",
    "        return word_vec\n",
    "    \n",
    "    def forward_pass(self,x):\n",
    "        h = np.dot(self.w1.T,x)\n",
    "        u = np.dot(self.w2.t,h)\n",
    "        y_c = self.softmax(u)\n",
    "        return y_c , h ,u\n",
    "        \n",
    "    def softmax(self,x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis = 0)\n",
    "\n",
    "\n",
    "    def backprop(self, e, h, x):\n",
    "        dl_dw2 = np.outer(h, e)  \n",
    "        dl_dw1 = np.outer(x, np.dot(self.w2, e.T))\n",
    "        self.w1 = self.w1 - (self.eta * dl_dw1)\n",
    "        self.w2 = self.w2 - (self.eta * dl_dw2)\n",
    "\n",
    "\n",
    "    def trianing(self,training_data):\n",
    "        self.w1=np.random.uniform(-1,1,(self.v_count,self.n))\n",
    "        self.w2=np.random.uniform(-1,1,(self.n,self.v_count))\n",
    "        for i in range(self.epochs):\n",
    "            self.loss = 0\n",
    "            for w_t , w_c in trainging_data:\n",
    "                y_pred , h ,u =self.forward_pass(w_t)\n",
    "                EI = np.sum([np.subtract(y_pred,word)for word in w_c],axixs=0)\n",
    "                self.backprop(EI,h,w_t)\n",
    "                self.loss += -np.sum([u[word.index(1)] for word in w_c]) + len(w_c) * np.log(np.sum(np.exp(u)))\n",
    "        print('Epoch:',i,\"loss:\",self.loss)\n",
    "\n",
    "    # input a word, returns a vector (if available)\n",
    "    def word_vec(self, word):\n",
    "        w_index = self.word_index[word]\n",
    "        v_w = self.w1[w_index]\n",
    "        return v_w\n",
    "\n",
    "\n",
    "    # input a vector, returns nearest word(s)\n",
    "    def vec_sim(self, vec, top_n):\n",
    "\n",
    "        # CYCLE THROUGH VOCAB\n",
    "        word_sim = {}\n",
    "        for i in range(self.v_count):\n",
    "            v_w2 = self.w1[i]\n",
    "            theta_num = np.dot(vec, v_w2)\n",
    "            theta_den = np.linalg.norm(vec) * np.linalg.norm(v_w2)\n",
    "            theta = theta_num / theta_den\n",
    "\n",
    "            word = self.index_word[i]\n",
    "            word_sim[word] = theta\n",
    "\n",
    "        words_sorted = sorted(word_sim.items(), key=lambda kv :kv[1] ,reverse=True)\n",
    "\n",
    "        for word, sim in words_sorted[:top_n]:\n",
    "            print (word, sim)\n",
    "            \n",
    "        pass\n",
    "\n",
    "    # input word, returns top [n] most similar words\n",
    "    def word_sim(self, word, top_n):\n",
    "        \n",
    "        w1_index = self.word_index[word]\n",
    "        v_w1 = self.w1[w1_index]\n",
    "\n",
    "        # CYCLE THROUGH VOCAB\n",
    "        word_sim = {}\n",
    "        for i in range(self.v_count):\n",
    "            v_w2 = self.w1[i]\n",
    "            theta_num = np.dot(v_w1, v_w2)\n",
    "            theta_den = np.linalg.norm(v_w1) * np.linalg.norm(v_w2)\n",
    "            theta = theta_num / theta_den\n",
    "\n",
    "            word = self.index_word[i]\n",
    "            word_sim[word] = theta\n",
    "\n",
    "        words_sorted = sorted(word_sim.items(), key=lambda kv :kv[1] , reverse=True)\n",
    "\n",
    "        for word, sim in words_sorted[:top_n]:\n",
    "            print (word, sim)\n",
    "            \n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-39-af995f710362>:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(trainging_data).astype(\"float64\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-bd36fd40ffe8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mw2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrainging_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw2v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_training_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# vec = w2v.word_vec(\"machine\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-39-af995f710362>\u001b[0m in \u001b[0;36mgenerate_training_data\u001b[1;34m(self, settings, corpus)\u001b[0m\n\u001b[0;32m     25\u001b[0m                         \u001b[0mword_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2onehot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                     \u001b[0mtrainging_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw_target\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword_context\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainging_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"float64\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mword2onehot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "w2v = word2vec()\n",
    "trainging_data = w2v.generate_training_data(settings,corpus)\n",
    "# vec = w2v.word_vec(\"machine\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1edc02d45a8b8ed3808a5fe551570e01c758028ee234a9214863c9ba4f4b6b20"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
