{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from collections import OrderedDict \n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text8():\n",
    "    with open(\"./text8.txt\", \"r\") as f:\n",
    "        corpus = f.read().strip(\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(corpus):\n",
    "    #由于英文单词出现在句首的时候经常要大写，所以我们把所有英文字符都转换为小写，\n",
    "    #以便对语料进行归一化处理（Apple vs apple等）\n",
    "    corpus = corpus.strip().lower()\n",
    "    corpus = corpus.split(\" \")\n",
    "\n",
    "    return corpus\n",
    "\n",
    "\n",
    "# #构造词典，统计每个词的频率，并根据频率将每个词转换为一个整数id\n",
    "# def build_dict(corpus):\n",
    "#     #首先统计每个不同词的频率（出现的次数），使用一个词典记录\n",
    "#     word_freq_dict = dict()\n",
    "#     for word in corpus:\n",
    "#         if word not in word_freq_dict:\n",
    "#             word_freq_dict[word] = 0\n",
    "#         word_freq_dict[word] += 1\n",
    "\n",
    "#     #将这个词典中的词，按照出现次数排序，出现次数越高，排序越靠前\n",
    "#     #一般来说，出现频率高的高频词往往是：I，the，you这种代词，而出现频率低的词，往往是一些名词，如：nlp\n",
    "#     word_freq_dict = sorted(word_freq_dict.items(), key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "#     #构造3个不同的词典，分别存储，\n",
    "#     #每个词到id的映射关系：word2id_dict\n",
    "#     #每个id出现的频率：word2id_freq\n",
    "#     #每个id到词典映射关系：id2word_dict\n",
    "#     word2id_dict = dict()\n",
    "#     word2id_freq = dict()\n",
    "#     id2word_dict = dict()\n",
    "\n",
    "#     #按照频率，从高到低，开始遍历每个单词，并为这个单词构造一个独一无二的id\n",
    "#     for word, freq in word_freq_dict:\n",
    "#         curr_id = len(word2id_dict)\n",
    "#         word2id_dict[word] = curr_id\n",
    "#         word2id_freq[word2id_dict[word]] = freq\n",
    "#         id2word_dict[curr_id] = word\n",
    "\n",
    "#     return word2id_freq, word2id_dict, id2word_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = data_preprocess(corpus)\n",
    "# print(corpus[:50])\n",
    "# word2id_freq, word2id_dict, id2word_dict = build_dict(corpus)\n",
    "# vocab_size = len(word2id_freq)\n",
    "# print(\"there are totoally %d different words in the corpus\" % vocab_size)\n",
    "# for _, (word, word_id) in zip(range(50), word2id_dict.items()):\n",
    "#     print(\"word %s, its id %d, its word freq %d\" % (word, word_id, word2id_freq[word_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vec():\n",
    "    def __init__ (self):\n",
    "        self.n = settings['n']\n",
    "        self.eta = settings['learning_rate']\n",
    "        self.epochs = settings['epochs']\n",
    "        self.window = settings['window_size']\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # GENERATE TRAINING DATA\n",
    "    def generate_training_data(self,corpus):\n",
    "\n",
    "        # GENERATE WORD COUNTS\n",
    "        word_counts = defaultdict(int)\n",
    "        for row in corpus:\n",
    "            for word in row:\n",
    "                word_counts[word] += 1\n",
    "\n",
    "        self.v_count = len(word_counts.keys())\n",
    "\n",
    "        # GENERATE LOOKUP DICTIONARIES\n",
    "        self.words_list = sorted(list(word_counts.keys()),reverse=False)\n",
    "        self.word_index = dict((word, i) for i, word in enumerate(self.words_list))\n",
    "        self.index_word = dict((i, word) for i, word in enumerate(self.words_list))\n",
    "        training_data = []\n",
    "        # CYCLE THROUGH EACH SENTENCE IN CORPUS\n",
    "        for sentence in corpus:\n",
    "            sent_len = len(sentence)\n",
    "\n",
    "            # CYCLE THROUGH EACH WORD IN SENTENCE\n",
    "            for i, word in enumerate(sentence):\n",
    "                \n",
    "                #w_target = sentence[i]\n",
    "                w_target = np.array(self.word2onehot(sentence[i]))\n",
    "\n",
    "                # CYCLE THROUGH CONTEXT WINDOW\n",
    "                w_context = np.array([0 for i in range(0, self.v_count)])\n",
    "                num=0\n",
    "                for j in range(i-self.window, i+self.window+1):\n",
    "                    if j!=i and j<=sent_len-1 and j>=0:\n",
    "                        w_context+=np.array(self.word2onehot(sentence[j]))\n",
    "                        num+=1\n",
    "                training_data.append([w_target, w_context/num])\n",
    "        return np.array(training_data)\n",
    "\n",
    "\n",
    "    # SOFTMAX ACTIVATION FUNCTION\n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "\n",
    "    # CONVERT WORD TO ONE HOT ENCODING\n",
    "    def word2onehot(self, word):\n",
    "        word_vec = [0 for i in range(0, self.v_count)]\n",
    "        word_index = self.word_index[word]\n",
    "        word_vec[word_index] = 1\n",
    "        return word_vec\n",
    "\n",
    "\n",
    "    # FORWARD PASS\n",
    "    def forward_pass(self, x):\n",
    "        h = np.dot(self.w1.T, x)\n",
    "        u = np.dot(self.w2.T, h)\n",
    "        y_c = self.softmax(u)\n",
    "        return y_c, h, u\n",
    "                \n",
    "\n",
    "    # BACKPROPAGATION\n",
    "    def backprop(self, e, h, x):\n",
    "        dl_dw2 = np.outer(h, e)  \n",
    "        dl_dw1 = np.outer(x, np.dot(self.w2, e.T))\n",
    "\n",
    "        # UPDATE WEIGHTS\n",
    "        self.w1 = self.w1 - (self.eta * dl_dw1)\n",
    "        self.w2 = self.w2 - (self.eta * dl_dw2)\n",
    "        pass\n",
    "\n",
    "\n",
    "    # TRAIN W2V model\n",
    "    def train(self, training_data):\n",
    "        # INITIALIZE WEIGHT MATRICES\n",
    "        self.w1 = np.random.uniform(-0.8, 0.8, (self.v_count, self.n))     # embedding matrix\n",
    "        self.w2 = np.random.uniform(-0.8, 0.8, (self.n, self.v_count))     # context matrix\n",
    "        \n",
    "        # CYCLE THROUGH EACH EPOCH\n",
    "        for i in range(0, self.epochs):\n",
    "\n",
    "            self.loss = 0\n",
    "\n",
    "            # CYCLE THROUGH EACH TRAINING SAMPLE\n",
    "            for j in training_data:\n",
    "                w_c = j[1]\n",
    "                w_t = j[0]\n",
    "                # FORWARD PASS\n",
    "                y_pred, h, u = self.forward_pass(w_c)\n",
    "                \n",
    "                # CALCULATE ERROR\n",
    "                EI = y_pred - w_t\n",
    "\n",
    "                # BACKPROPAGATION\n",
    "                self.backprop(EI, h, w_t)\n",
    "\n",
    "                # CALCULATE LOSS\n",
    "                self.loss += np.max(y_pred)\n",
    "                #self.loss += -2*np.log(len(w_c)) -np.sum([u[word.index(1)] for word in w_c]) + (len(w_c) * np.log(np.sum(np.exp(u))))\n",
    "\n",
    "            print ('EPOCH:',i, 'LOSS:', self.loss)\n",
    "        # print(self.w1)\n",
    "        # print(self.w2)\n",
    "\n",
    "\n",
    "    # input a word, returns a vector (if available)\n",
    "    def word_vec(self, word):\n",
    "        w_index = self.word_index[word]\n",
    "        v_w = self.w1[w_index]\n",
    "        return self.softmax(v_w)\n",
    "\n",
    "\n",
    "\n",
    "    # input a vector, returns nearest word(s)\n",
    "    def vec_sim(self, vec, top_n):\n",
    "\n",
    "        # CYCLE THROUGH VOCAB\n",
    "        word_sim = {}\n",
    "        for i in range(self.v_count):\n",
    "            v_w2 = self.w1[i]\n",
    "            theta_num = np.dot(vec, v_w2)\n",
    "            theta_den = np.linalg.norm(vec) * np.linalg.norm(v_w2)\n",
    "            theta = theta_num / theta_den\n",
    "\n",
    "            word = self.index_word[i]\n",
    "            word_sim[word] = theta\n",
    "\n",
    "        words_sorted = sorted(word_sim.items(), key=lambda kv :kv[1] , reverse=True)\n",
    "\n",
    "        for word, sim in words_sorted[:top_n]:\n",
    "            print (word, sim)\n",
    "            \n",
    "        pass\n",
    "\n",
    "    # input word, returns top [n] most similar words\n",
    "    def word_sim(self, word, top_n):\n",
    "        \n",
    "        w1_index = self.word_index[word]\n",
    "        v_w1 = self.w1[w1_index]\n",
    "\n",
    "        # CYCLE THROUGH VOCAB\n",
    "        word_sim = {}\n",
    "        for i in range(self.v_count):\n",
    "            v_w2 = self.w1[i]\n",
    "            theta_num = np.dot(v_w1, v_w2)\n",
    "            theta_den = np.linalg.norm(v_w1) * np.linalg.norm(v_w2)\n",
    "            theta = theta_num / theta_den\n",
    "\n",
    "            word = self.index_word[i]\n",
    "            word_sim[word] = theta\n",
    "\n",
    "        words_sorted = sorted(word_sim.items(), key=lambda kv :kv[1] , reverse=True)\n",
    "\n",
    "        for word, sim in words_sorted[:top_n]:\n",
    "            print (word, sim)\n",
    "            \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {}\n",
    "settings['n'] = 20                 # dimension of word embeddings\n",
    "settings['window_size'] = 2         # context window +/- center word\n",
    "settings['epochs'] = 300       # number of training epochs\n",
    "settings['neg_samp'] = 10           # number of negative words to use during training\n",
    "settings['learning_rate'] = 0.1    # learning rate\n",
    "np.random.seed(0)                   # set the seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_text8()\n",
    "corpus = [data_preprocess(corpus[:10000])]\n",
    "# print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 LOSS: 55.31009358492388\n",
      "EPOCH: 1 LOSS: 141.45011712647894\n",
      "EPOCH: 2 LOSS: 223.42116476605153\n",
      "EPOCH: 3 LOSS: 289.5856682138619\n",
      "EPOCH: 4 LOSS: 348.5551096161694\n",
      "EPOCH: 5 LOSS: 399.96460666980187\n",
      "EPOCH: 6 LOSS: 442.0149012149577\n",
      "EPOCH: 7 LOSS: 472.94429878122514\n",
      "EPOCH: 8 LOSS: 509.7104456417818\n",
      "EPOCH: 9 LOSS: 546.5729570769716\n",
      "EPOCH: 10 LOSS: 577.5777754655145\n",
      "EPOCH: 11 LOSS: 600.0240127719813\n",
      "EPOCH: 12 LOSS: 612.6819109883329\n",
      "EPOCH: 13 LOSS: 633.6250620494292\n",
      "EPOCH: 14 LOSS: 659.2464257200375\n",
      "EPOCH: 15 LOSS: 684.1533967353755\n",
      "EPOCH: 16 LOSS: 697.6498154881535\n",
      "EPOCH: 17 LOSS: 722.7551773308976\n",
      "EPOCH: 18 LOSS: 734.7615626267316\n",
      "EPOCH: 19 LOSS: 765.0605050190073\n",
      "EPOCH: 20 LOSS: 764.6785517659976\n",
      "EPOCH: 21 LOSS: 785.1556726890545\n",
      "EPOCH: 22 LOSS: 784.449440973969\n",
      "EPOCH: 23 LOSS: 797.8221859836378\n",
      "EPOCH: 24 LOSS: 802.5830065485978\n",
      "EPOCH: 25 LOSS: 822.0444825658482\n",
      "EPOCH: 26 LOSS: 837.8661529776679\n",
      "EPOCH: 27 LOSS: 854.7421973729838\n",
      "EPOCH: 28 LOSS: 850.1737486815\n",
      "EPOCH: 29 LOSS: 858.2166542362671\n",
      "EPOCH: 30 LOSS: 867.4561019739061\n",
      "EPOCH: 31 LOSS: 877.5847049971277\n",
      "EPOCH: 32 LOSS: 879.6106578890956\n",
      "EPOCH: 33 LOSS: 885.1032556121384\n",
      "EPOCH: 34 LOSS: 896.3011549831677\n",
      "EPOCH: 35 LOSS: 905.9995985357707\n",
      "EPOCH: 36 LOSS: 904.3209659855828\n",
      "EPOCH: 37 LOSS: 916.4680222161741\n",
      "EPOCH: 38 LOSS: 919.1096215202754\n",
      "EPOCH: 39 LOSS: 918.4968267199479\n",
      "EPOCH: 40 LOSS: 923.9699518322606\n",
      "EPOCH: 41 LOSS: 921.3781034223236\n",
      "EPOCH: 42 LOSS: 926.6018259030167\n",
      "EPOCH: 43 LOSS: 919.5731153214\n",
      "EPOCH: 44 LOSS: 927.5185110435013\n",
      "EPOCH: 45 LOSS: 931.094425780751\n",
      "EPOCH: 46 LOSS: 922.6777570366611\n",
      "EPOCH: 47 LOSS: 927.2903607102289\n",
      "EPOCH: 48 LOSS: 933.3801585903923\n",
      "EPOCH: 49 LOSS: 928.7807745660497\n",
      "EPOCH: 50 LOSS: 938.9539898472214\n",
      "EPOCH: 51 LOSS: 942.1658157208784\n",
      "EPOCH: 52 LOSS: 950.8444311851886\n",
      "EPOCH: 53 LOSS: 950.4217971422015\n",
      "EPOCH: 54 LOSS: 964.267049983429\n",
      "EPOCH: 55 LOSS: 966.9763692443358\n",
      "EPOCH: 56 LOSS: 973.8287340343569\n",
      "EPOCH: 57 LOSS: 974.8407167070312\n",
      "EPOCH: 58 LOSS: 978.5917158111756\n",
      "EPOCH: 59 LOSS: 971.8659226250096\n",
      "EPOCH: 60 LOSS: 976.0102327708101\n",
      "EPOCH: 61 LOSS: 978.9592964626718\n",
      "EPOCH: 62 LOSS: 964.7140855020576\n",
      "EPOCH: 63 LOSS: 970.0902954134245\n",
      "EPOCH: 64 LOSS: 967.8935104558954\n",
      "EPOCH: 65 LOSS: 970.6011768640371\n",
      "EPOCH: 66 LOSS: 979.5087539004755\n",
      "EPOCH: 67 LOSS: 978.9633259034244\n",
      "EPOCH: 68 LOSS: 984.3502028670423\n",
      "EPOCH: 69 LOSS: 984.290151567082\n",
      "EPOCH: 70 LOSS: 988.6129146239571\n",
      "EPOCH: 71 LOSS: 981.6625053498301\n",
      "EPOCH: 72 LOSS: 987.3940736688519\n",
      "EPOCH: 73 LOSS: 990.8203308357434\n",
      "EPOCH: 74 LOSS: 978.363293536897\n",
      "EPOCH: 75 LOSS: 980.7518558212416\n",
      "EPOCH: 76 LOSS: 979.8465230749564\n",
      "EPOCH: 77 LOSS: 969.7964620921639\n",
      "EPOCH: 78 LOSS: 978.6994700619916\n",
      "EPOCH: 79 LOSS: 972.5088639295718\n",
      "EPOCH: 80 LOSS: 977.0406869635486\n",
      "EPOCH: 81 LOSS: 973.5894896067665\n",
      "EPOCH: 82 LOSS: 975.3823384056349\n",
      "EPOCH: 83 LOSS: 981.7150769091861\n",
      "EPOCH: 84 LOSS: 977.2422926535143\n",
      "EPOCH: 85 LOSS: 980.9601054680725\n",
      "EPOCH: 86 LOSS: 979.191154153782\n",
      "EPOCH: 87 LOSS: 973.9403371504269\n",
      "EPOCH: 88 LOSS: 982.0953926708452\n",
      "EPOCH: 89 LOSS: 981.3699644166355\n",
      "EPOCH: 90 LOSS: 991.3775699132344\n",
      "EPOCH: 91 LOSS: 988.6700148902268\n",
      "EPOCH: 92 LOSS: 999.1070229159742\n",
      "EPOCH: 93 LOSS: 1003.6818270797338\n",
      "EPOCH: 94 LOSS: 1006.7632005888073\n",
      "EPOCH: 95 LOSS: 1003.1439556555283\n",
      "EPOCH: 96 LOSS: 1006.5143819715133\n",
      "EPOCH: 97 LOSS: 999.1415468433806\n",
      "EPOCH: 98 LOSS: 1011.7255353026179\n",
      "EPOCH: 99 LOSS: 1003.0170170696163\n",
      "EPOCH: 100 LOSS: 1009.2453914456273\n",
      "EPOCH: 101 LOSS: 1006.8123860885596\n",
      "EPOCH: 102 LOSS: 1014.5826760838011\n",
      "EPOCH: 103 LOSS: 1012.4667749386844\n",
      "EPOCH: 104 LOSS: 1007.5429282400373\n",
      "EPOCH: 105 LOSS: 1021.2117737680773\n",
      "EPOCH: 106 LOSS: 1011.9538234765939\n",
      "EPOCH: 107 LOSS: 1013.192184677026\n",
      "EPOCH: 108 LOSS: 1012.4432561794634\n",
      "EPOCH: 109 LOSS: 1005.8041466206921\n",
      "EPOCH: 110 LOSS: 1008.648760981853\n",
      "EPOCH: 111 LOSS: 1004.8742009776357\n",
      "EPOCH: 112 LOSS: 1011.1449326077459\n",
      "EPOCH: 113 LOSS: 1006.6174063664918\n",
      "EPOCH: 114 LOSS: 1005.0502905557689\n",
      "EPOCH: 115 LOSS: 1012.2048722424563\n",
      "EPOCH: 116 LOSS: 1000.2098047725234\n",
      "EPOCH: 117 LOSS: 1000.7075312649587\n",
      "EPOCH: 118 LOSS: 1003.3402407377135\n",
      "EPOCH: 119 LOSS: 1002.6334611214811\n",
      "EPOCH: 120 LOSS: 1011.470021939231\n",
      "EPOCH: 121 LOSS: 1009.6866575889218\n",
      "EPOCH: 122 LOSS: 1020.3573019510479\n",
      "EPOCH: 123 LOSS: 1017.8276480837825\n",
      "EPOCH: 124 LOSS: 1018.110430436323\n",
      "EPOCH: 125 LOSS: 1022.3750567961486\n",
      "EPOCH: 126 LOSS: 1021.3219881325449\n",
      "EPOCH: 127 LOSS: 1030.7667990110624\n",
      "EPOCH: 128 LOSS: 1013.9338583926406\n",
      "EPOCH: 129 LOSS: 1018.2673195439602\n",
      "EPOCH: 130 LOSS: 1027.0623673451553\n",
      "EPOCH: 131 LOSS: 1024.7113162605708\n",
      "EPOCH: 132 LOSS: 1018.411038838141\n",
      "EPOCH: 133 LOSS: 1024.236772088251\n",
      "EPOCH: 134 LOSS: 1012.787865899274\n",
      "EPOCH: 135 LOSS: 1022.573867249423\n",
      "EPOCH: 136 LOSS: 1019.1282741450153\n",
      "EPOCH: 137 LOSS: 1022.0790491448332\n",
      "EPOCH: 138 LOSS: 1016.510040950999\n",
      "EPOCH: 139 LOSS: 1021.2408710439689\n",
      "EPOCH: 140 LOSS: 1015.4868732199001\n",
      "EPOCH: 141 LOSS: 1015.7570559485762\n",
      "EPOCH: 142 LOSS: 1012.8568304812519\n",
      "EPOCH: 143 LOSS: 1014.9001633683447\n",
      "EPOCH: 144 LOSS: 1013.2136669337648\n",
      "EPOCH: 145 LOSS: 1014.6134665560808\n",
      "EPOCH: 146 LOSS: 1003.7872796223421\n",
      "EPOCH: 147 LOSS: 1008.8411562076919\n",
      "EPOCH: 148 LOSS: 1013.8822459633451\n",
      "EPOCH: 149 LOSS: 998.375668882778\n",
      "EPOCH: 150 LOSS: 1011.8933341736143\n",
      "EPOCH: 151 LOSS: 1005.6296590555469\n",
      "EPOCH: 152 LOSS: 1018.0261764635533\n",
      "EPOCH: 153 LOSS: 1000.1741675101696\n",
      "EPOCH: 154 LOSS: 1015.9980720946248\n",
      "EPOCH: 155 LOSS: 1006.1630352415258\n",
      "EPOCH: 156 LOSS: 1013.4082320503824\n",
      "EPOCH: 157 LOSS: 1003.9750575001217\n",
      "EPOCH: 158 LOSS: 1011.6176917091067\n",
      "EPOCH: 159 LOSS: 1008.1058320846217\n",
      "EPOCH: 160 LOSS: 1004.6807057219982\n",
      "EPOCH: 161 LOSS: 1004.0400192463522\n",
      "EPOCH: 162 LOSS: 1007.6038902377209\n",
      "EPOCH: 163 LOSS: 1006.0088396651247\n",
      "EPOCH: 164 LOSS: 1010.1392796253059\n",
      "EPOCH: 165 LOSS: 1008.957965168657\n",
      "EPOCH: 166 LOSS: 1014.9264109322102\n",
      "EPOCH: 167 LOSS: 1013.1544352447847\n",
      "EPOCH: 168 LOSS: 1015.6170396724659\n",
      "EPOCH: 169 LOSS: 1013.5479397960654\n",
      "EPOCH: 170 LOSS: 1024.2001150713722\n",
      "EPOCH: 171 LOSS: 1018.1050558628993\n",
      "EPOCH: 172 LOSS: 1019.4820014222914\n",
      "EPOCH: 173 LOSS: 1021.6268991781453\n",
      "EPOCH: 174 LOSS: 1018.9193101441676\n",
      "EPOCH: 175 LOSS: 1020.9020249704372\n",
      "EPOCH: 176 LOSS: 1027.421204534356\n",
      "EPOCH: 177 LOSS: 1018.7649773509964\n",
      "EPOCH: 178 LOSS: 1019.8389098265789\n",
      "EPOCH: 179 LOSS: 1019.4191576122951\n",
      "EPOCH: 180 LOSS: 1026.5828092687345\n",
      "EPOCH: 181 LOSS: 1021.7676188317917\n",
      "EPOCH: 182 LOSS: 1027.3477711939308\n",
      "EPOCH: 183 LOSS: 1027.7718093603846\n",
      "EPOCH: 184 LOSS: 1018.3295185245288\n",
      "EPOCH: 185 LOSS: 1029.6615998884192\n",
      "EPOCH: 186 LOSS: 1013.4338273674944\n",
      "EPOCH: 187 LOSS: 1018.3210699088291\n",
      "EPOCH: 188 LOSS: 1014.8321972633386\n",
      "EPOCH: 189 LOSS: 1009.6556855325155\n",
      "EPOCH: 190 LOSS: 1017.5258643699318\n",
      "EPOCH: 191 LOSS: 1006.1714115293194\n",
      "EPOCH: 192 LOSS: 1012.3085685526589\n",
      "EPOCH: 193 LOSS: 1024.5740488742679\n",
      "EPOCH: 194 LOSS: 1011.8182328930425\n",
      "EPOCH: 195 LOSS: 1018.3947877544767\n",
      "EPOCH: 196 LOSS: 1017.9436927457007\n",
      "EPOCH: 197 LOSS: 1006.9537013771602\n",
      "EPOCH: 198 LOSS: 1018.946132607969\n",
      "EPOCH: 199 LOSS: 1018.8551370987248\n",
      "EPOCH: 200 LOSS: 1015.4350152317803\n",
      "EPOCH: 201 LOSS: 1016.3319444129393\n",
      "EPOCH: 202 LOSS: 1015.0957608802318\n",
      "EPOCH: 203 LOSS: 1024.9136354875404\n",
      "EPOCH: 204 LOSS: 1021.0936898542008\n",
      "EPOCH: 205 LOSS: 1022.5049921484947\n",
      "EPOCH: 206 LOSS: 1023.2321215794612\n",
      "EPOCH: 207 LOSS: 1033.8855476002618\n",
      "EPOCH: 208 LOSS: 1025.8845838615518\n",
      "EPOCH: 209 LOSS: 1033.5207504032442\n",
      "EPOCH: 210 LOSS: 1030.1237404727513\n",
      "EPOCH: 211 LOSS: 1027.5419745715417\n",
      "EPOCH: 212 LOSS: 1043.0973351968091\n",
      "EPOCH: 213 LOSS: 1029.9504434963712\n",
      "EPOCH: 214 LOSS: 1031.2787220169434\n",
      "EPOCH: 215 LOSS: 1024.982551307933\n",
      "EPOCH: 216 LOSS: 1029.58760154702\n",
      "EPOCH: 217 LOSS: 1017.9429077389603\n",
      "EPOCH: 218 LOSS: 1029.0277547791898\n",
      "EPOCH: 219 LOSS: 1020.635961343932\n",
      "EPOCH: 220 LOSS: 1032.9095114604672\n",
      "EPOCH: 221 LOSS: 1033.695947691618\n",
      "EPOCH: 222 LOSS: 1035.7305129313263\n",
      "EPOCH: 223 LOSS: 1037.0316971439597\n",
      "EPOCH: 224 LOSS: 1041.4464356605993\n",
      "EPOCH: 225 LOSS: 1027.4807357696575\n",
      "EPOCH: 226 LOSS: 1050.5623712756462\n",
      "EPOCH: 227 LOSS: 1037.6422095102319\n",
      "EPOCH: 228 LOSS: 1058.3343453553696\n",
      "EPOCH: 229 LOSS: 1036.9764634585024\n",
      "EPOCH: 230 LOSS: 1051.3185501266437\n",
      "EPOCH: 231 LOSS: 1044.5389407297541\n",
      "EPOCH: 232 LOSS: 1037.707987809024\n",
      "EPOCH: 233 LOSS: 1034.1904135828465\n",
      "EPOCH: 234 LOSS: 1039.9678293610427\n",
      "EPOCH: 235 LOSS: 1047.866212732108\n",
      "EPOCH: 236 LOSS: 1041.7523284794372\n",
      "EPOCH: 237 LOSS: 1041.0778159287706\n",
      "EPOCH: 238 LOSS: 1046.40072888747\n",
      "EPOCH: 239 LOSS: 1055.8327567554254\n",
      "EPOCH: 240 LOSS: 1041.4273936808527\n",
      "EPOCH: 241 LOSS: 1041.5469830874792\n",
      "EPOCH: 242 LOSS: 1041.263026846095\n",
      "EPOCH: 243 LOSS: 1048.4255457011977\n",
      "EPOCH: 244 LOSS: 1045.8342352285317\n",
      "EPOCH: 245 LOSS: 1055.6273967445436\n",
      "EPOCH: 246 LOSS: 1045.9928082870877\n",
      "EPOCH: 247 LOSS: 1053.1456014528012\n",
      "EPOCH: 248 LOSS: 1048.2464037771822\n",
      "EPOCH: 249 LOSS: 1041.2650901026946\n",
      "EPOCH: 250 LOSS: 1044.3098681207082\n",
      "EPOCH: 251 LOSS: 1043.187366878535\n",
      "EPOCH: 252 LOSS: 1044.6721981739222\n",
      "EPOCH: 253 LOSS: 1056.4730858843648\n",
      "EPOCH: 254 LOSS: 1042.5083635901674\n",
      "EPOCH: 255 LOSS: 1055.4067294895378\n",
      "EPOCH: 256 LOSS: 1049.4805082550522\n",
      "EPOCH: 257 LOSS: 1046.3399989832865\n",
      "EPOCH: 258 LOSS: 1044.588740381943\n",
      "EPOCH: 259 LOSS: 1047.3663389117628\n",
      "EPOCH: 260 LOSS: 1040.5881817944519\n",
      "EPOCH: 261 LOSS: 1043.1661882486007\n",
      "EPOCH: 262 LOSS: 1042.2797976959232\n",
      "EPOCH: 263 LOSS: 1044.6816048891808\n",
      "EPOCH: 264 LOSS: 1042.473631832163\n",
      "EPOCH: 265 LOSS: 1035.8840009776984\n",
      "EPOCH: 266 LOSS: 1040.0180311346755\n",
      "EPOCH: 267 LOSS: 1036.7107175623519\n",
      "EPOCH: 268 LOSS: 1046.0272430155342\n",
      "EPOCH: 269 LOSS: 1040.1056433537817\n",
      "EPOCH: 270 LOSS: 1028.9670485812505\n",
      "EPOCH: 271 LOSS: 1038.0614153062984\n",
      "EPOCH: 272 LOSS: 1041.3814934377006\n",
      "EPOCH: 273 LOSS: 1035.1082171538185\n",
      "EPOCH: 274 LOSS: 1043.938091825347\n",
      "EPOCH: 275 LOSS: 1039.0315806071362\n",
      "EPOCH: 276 LOSS: 1049.8453016276806\n",
      "EPOCH: 277 LOSS: 1049.5598543915287\n",
      "EPOCH: 278 LOSS: 1056.4342782622234\n",
      "EPOCH: 279 LOSS: 1049.332911005434\n",
      "EPOCH: 280 LOSS: 1058.0175675361738\n",
      "EPOCH: 281 LOSS: 1050.248911255977\n",
      "EPOCH: 282 LOSS: 1065.5635732582907\n",
      "EPOCH: 283 LOSS: 1047.4495745851855\n",
      "EPOCH: 284 LOSS: 1053.8681791558906\n",
      "EPOCH: 285 LOSS: 1064.825104019254\n",
      "EPOCH: 286 LOSS: 1049.608843978758\n",
      "EPOCH: 287 LOSS: 1055.3874633078037\n",
      "EPOCH: 288 LOSS: 1045.5118357707777\n",
      "EPOCH: 289 LOSS: 1049.1324818264736\n",
      "EPOCH: 290 LOSS: 1054.4336746580616\n",
      "EPOCH: 291 LOSS: 1058.0272820764076\n",
      "EPOCH: 292 LOSS: 1054.9035695413131\n",
      "EPOCH: 293 LOSS: 1053.8849996352599\n",
      "EPOCH: 294 LOSS: 1065.3499307329928\n",
      "EPOCH: 295 LOSS: 1053.4586025686788\n",
      "EPOCH: 296 LOSS: 1066.8013757325518\n",
      "EPOCH: 297 LOSS: 1053.3133489249205\n",
      "EPOCH: 298 LOSS: 1057.1994279669236\n",
      "EPOCH: 299 LOSS: 1067.828634784387\n"
     ]
    }
   ],
   "source": [
    "w2v = word2vec()\n",
    "\n",
    "training_data = w2v.generate_training_data(corpus)\n",
    "\n",
    "w2v.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.15310406e-08 6.71997606e-12 7.90544248e-09 2.09505141e-02\n",
      " 3.01808844e-09 1.19278395e-07 5.86240899e-01 1.86539216e-13\n",
      " 3.92752523e-01 1.70656192e-10 3.60911077e-11 1.06090043e-05\n",
      " 6.17848159e-11 3.53067709e-08 3.23412595e-14 1.49427662e-12\n",
      " 7.54634302e-15 4.16399784e-09 4.50506427e-05 2.22658538e-07]\n",
      "free 0.9999999999999999\n",
      "did 0.5575728091186204\n",
      "truly 0.5340838911766466\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(w2v.word_vec('liberty'))\n",
    "\n",
    "print(w2v.word_sim('free',3))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1edc02d45a8b8ed3808a5fe551570e01c758028ee234a9214863c9ba4f4b6b20"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
